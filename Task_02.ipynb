{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamoud-Hmd/ADL_google_colab/blob/main/Task_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsqKM5DLlLRm"
      },
      "source": [
        "# Task 2 – MLP, Learning rate, Overfitting, and Hyper-parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTID5Dfh4ZbL"
      },
      "source": [
        "### <span style=\"color:red\">Deadline Tuesday, June 3, 2025 at 11:59 p.m<span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaoMWjbZdCLf"
      },
      "source": [
        "# IMPORTANT SUBMISSION INSTRUCTIONS\n",
        "\n",
        "- When you're done, download the notebook and rename it to task02_[name].ipynb\n",
        "- Only submit the ipynb file, no other file is required.\n",
        "- The deadline is strict.\n",
        "- Minimal requirement for passing: solving all code cells.\n",
        "\n",
        "Implementation\n",
        "- Do not change the cells which are marked as \"Do not change\", similarly write your solution to the marked cells. Do not create additional cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8RW2QHHFQYe"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this task you will implement an MLP model for virtual sensing using the flood dataset.\n",
        "The objectives are:\n",
        "- Implementing an MLP model via TensorFlow Functional API.\n",
        "- Getting more familiar with model fitting and overfitting.\n",
        "- Implementing early stopping.\n",
        "- Exploring hyperparameters and their influence.\n",
        "- Selecting model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhw-7EA-10Xm"
      },
      "source": [
        "## Tutorials\n",
        "\n",
        "Some python libraries are required to accomplish the tasks assigned in this homework. If you feel like you need to follow a tutorial before, feel free to do so:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMyEloGz__BZ"
      },
      "source": [
        "*   [Scikit-learn Tutorials](https://www.tensorflow.org/tutorials)\n",
        "*   [TensorFlow Tutorials](https://scikit-learn.org/stable/tutorial/index.html)\n",
        "*   [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ActTWMg4XZ5v"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LArjND15dGNh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, optimizers, losses, callbacks\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "SEED = 24\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpqN-O0F_atv"
      },
      "source": [
        "## System checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1r5yPHY_hsl"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "cpus = tf.config.list_physical_devices('CPU')\n",
        "print(gpus)\n",
        "print(cpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elUTjY2H4ZbM"
      },
      "source": [
        "Choose your device for computation. CPU or one of your CUDA devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMCbCpAFao6q"
      },
      "outputs": [],
      "source": [
        "tf.config.set_visible_devices(gpus, 'GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Iz73Go4ZbN"
      },
      "source": [
        "# Subtask 2.1\n",
        "\n",
        "## Backpropagation – Chain Rule\n",
        "\n",
        "### Model\n",
        "Let us assume an MLP with one hidden layer containing one neuron with `sigmoid` function ($S(x) = 1 / (1 + e^{-x})$) as the activation function. The input and the output dimensions are equal to one. The activation function for the input and output layers is `linear`.\n",
        "The mapping $f: x \\to \\tilde{y}$ from the input $x$ to output $\\tilde{y}$ can be written as:\n",
        "\\begin{equation}\n",
        "\\tilde{y} = f(x) = w_2 S(w_1 x)\n",
        "\\end{equation}\n",
        "where $w_1$ and $w_2$ are the weights of the model. Note that we do not have bias for this example. Let us consider mean-squared error as the loss function. The loss $\\ell$ can be obtained as:\n",
        "\\begin{equation}\n",
        "\\ell = (y - \\tilde{y})^2 = (y - w_2 S(w_1 x))^2\n",
        "\\end{equation}\n",
        "where $y$ denotes the reference label. Let's initialize the weights as $w_1 = 0.1$ and $w_2 = 0.1$.\n",
        "\n",
        "### Data\n",
        "\n",
        "Let's assume we have a data set containing three samples as: $x = [1.0, 2.0, 3.0]^{T}$ and $y = [1.0, 4.0, 9.0]^{T}$.\n",
        "\n",
        "#### TODO\n",
        " - Perform gradient descent by hand with a learning rate of 0.1. Train the model for 1 epoch with the batch size of 1.\n",
        " - Report the prediction $\\tilde{y}$, loss, gradients, and the updated weights at every iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTM1mScT4ZbN"
      },
      "source": [
        "<span style='color:red'>**Your answer:**</span>\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kRRa2CN4ZbN"
      },
      "source": [
        "#### TODO\n",
        "- Define two functions that compute the gradients of the loss with respect to $w_1$ and $w_2$ using analytical differentiation and chain rule.\n",
        "- Use the functions you defined and train the model again using gradient descent and the same learning rate.\n",
        "- Report the prediction $\\tilde{y}$, loss, gradients, and the updated weights at every iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZJpSzQu4ZbN"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8TNV5YK4ZbN"
      },
      "source": [
        "#### TODO\n",
        "- Use automatic differentiation (AD) (`tf.GradientTape`) to compute the gradients.\n",
        "- Train the model again using the same learning rate, epoch, and batch size.\n",
        "- Report the prediction $\\tilde{y}$, loss, gradients, and the updated weights at every iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UQLDlY84ZbN"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOKbpUDU4ZbN"
      },
      "source": [
        "# Subtask 2.2\n",
        "\n",
        "In this task, we consider the data from **Margarethenklippe** as the input and train an MLP that gives the information about **Sennhuette** as the output. For each of the sensors, we have the water level, **W**, and the mass flow rate, **Q**, every 15 minutes.\n",
        "## Pre-processing\n",
        "\n",
        "#### TODO\n",
        " - Load the flood dataset (`Flood_Data.csv`) using *pandas* library.\n",
        " - Drop the first four columns of the DataFrame and columns containing NaNs.\n",
        " - Split the data into inputs and outputs.\n",
        " - Shuffle the data and get the NumPy arrays.\n",
        " - Split the data to 75% for training, 15% for validation, and 15% for testing.\n",
        " - Standardize inputs and outputs.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-cCRPNp4ZbN"
      },
      "outputs": [],
      "source": [
        "# TODO load the flood dataset using pandas\n",
        "\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5imM0Tl4ZbN"
      },
      "outputs": [],
      "source": [
        "# TODO split the data into inputs X and outputs y\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n",
        "\n",
        "N_samples = X.shape[0] # number of samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6PXqsfA4ZbN"
      },
      "outputs": [],
      "source": [
        "# TODO shuffle the data and get the NumPy arrays:\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n",
        "\n",
        "# TODO split the data to train, validation, and test:\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w4VmX5c4ZbN"
      },
      "source": [
        "Let us define a class for standardizing the data. We will implement a function that applies the scaling (`apply`) and a function that maps the scaled data back into the original scale (`apply_reverse`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cr1ZSOO4ZbN"
      },
      "outputs": [],
      "source": [
        "# TODO create a class for standardizing the data.\n",
        "\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8__Qpa_4ZbO"
      },
      "outputs": [],
      "source": [
        "# TODO standardize the data using the class you defined in the cell above.\n",
        "\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g05ayv2I4ZbO"
      },
      "source": [
        "# Subtask 2.3\n",
        "\n",
        "## Implementing an MLP using TensorFlow Functional API.\n",
        "\n",
        "#### TODO\n",
        "\n",
        "- Implement an MLP using TensorFlow Functional API.\n",
        "- Print the model architecture using `model.summary()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFKXQIpQ4ZbO"
      },
      "outputs": [],
      "source": [
        "# TODO implement the model\n",
        "\n",
        "def MLP(N_hidden_layers, N_neurons, activation):\n",
        "    input_layer = ...\n",
        "    x = input_layer\n",
        "    for i in range(N_hidden_layers):\n",
        "        x = ...\n",
        "    output_layer = ...\n",
        "\n",
        "    model = models.Model(input_layer, output_layer)\n",
        "    return model\n",
        "\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n",
        "model ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V82l3JAW4ZbO"
      },
      "outputs": [],
      "source": [
        "# TODO print the model architecture\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF4AuTcg4ZbO"
      },
      "source": [
        "# Subtask 2.4\n",
        "\n",
        "## Optimizer, loss function and training\n",
        "\n",
        "#### TODO\n",
        "\n",
        "- Define the optimizer and the loss function using `tf.keras.optimizers` and `tf.keras.losses`, respectively.\n",
        "    - Use stochastic gradient descent (SGD) with a learning rate of 1.0 as the optimizer.\n",
        "    - Use mean-squared error as the loss function.\n",
        "- Compile the model using the optimizer and the loss.\n",
        "- Train the model and get the history of training and validation losses.\n",
        "    - Train the model for 10 epochs.\n",
        "    - Use a batch size of 512.\n",
        "- Plot the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXRXRSUy4ZbO"
      },
      "outputs": [],
      "source": [
        "# TODO define the optimizer and the loss function\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n",
        "\n",
        "# TODO compile the model\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXg_Wyj94ZbO"
      },
      "outputs": [],
      "source": [
        "# TODO train the model\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q99vqP054ZbO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_learning_curves(hist):\n",
        "    epochs = np.arange(0, len(hist.history['loss'])) + 1\n",
        "    sns.set(style='ticks')\n",
        "    fig, ax = plt.subplots(1, 1, figsize = (5, 4))\n",
        "    ax.plot(epochs, hist.history['loss'], label = 'Training loss', marker = 'o', ls = '--')\n",
        "    ax.plot(epochs, hist.history['val_loss'], label = 'Validation loss', marker = 'o', ls = '--')\n",
        "\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title('Learning curves')\n",
        "    ax.legend()\n",
        "    sns.despine(trim=True, offset=5)\n",
        "\n",
        "# TODO Use the function above to plot the learning curves\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk_sFA8u4ZbO"
      },
      "source": [
        "**TODO Your answer here**\n",
        "\n",
        "Does the model learn properly? Why?\n",
        "\n",
        "**TODO Your answer here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9tdIxSH4ZbO"
      },
      "source": [
        "<span style='color:red'>**Your answer:**</span>\n",
        "\n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUav-Llu4ZbO"
      },
      "source": [
        "# Subtask 2.5\n",
        "\n",
        "## Learning rate\n",
        "\n",
        "#### TODO\n",
        "- Decrease the learning rate logarithmically, i.e. by a factor of 10, until your model starts to train.\n",
        "    - Train the model for 10 epochs.\n",
        "    - Use a batch size of 512.\n",
        "- Plot the training curves of the loss and the accuracies as in Subtask 1.3. Use the function defined above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz7VluIX4ZbO"
      },
      "outputs": [],
      "source": [
        "# learning rate = 0.1\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLghFlkI4ZbO"
      },
      "outputs": [],
      "source": [
        "# learning rate = 0.01\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhBTjHDe4ZbP"
      },
      "outputs": [],
      "source": [
        "# learning rate = 0.001\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GchS4dcF4ZbP"
      },
      "outputs": [],
      "source": [
        "# learning rate = 1e-4\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeT2o5aM4ZbP"
      },
      "source": [
        "## Model selection\n",
        "\n",
        "Is it the lowest validation loss that one can achieve? Probably not. So here are your TODOs:\n",
        "\n",
        "#### TODO:\n",
        "- Train the model for 100 epochs and plot the learning curves. Use learning rate of 0.01.\n",
        "\n",
        "#### TODO from now on, for all subsequent tasks:\n",
        "- Print the overall best validation loss and the epoch at which it occurred of.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci8YT6g54ZbP"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MIIpK7e4ZbP"
      },
      "source": [
        "**TODO Your answer here**\n",
        "\n",
        "Answer the following questions in written form:\n",
        "\n",
        "1. Does the training loss decrease after each epoch? Why does it? // Why does it not?\n",
        "1. Does the validation loss decrease after each epoch? Why does it? // Why does it not? (For your answer to be sufficient, you should describe fluctuations and discuss the overall minimum of the curve.)\n",
        "1. At which epoch was your model best? I.e. if you had saved your model after each training epoch, which one would you use to make predictions to unseen samples (e.g. from the test set)? Why? (For your answer to be sufficient: Also discuss what this means in terms of overfitting)\n",
        "\n",
        "**TODO Your answer here**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l2-M5gZ4ZbP"
      },
      "source": [
        "<span style='color:red'>**Your answer:**</span>\n",
        "\n",
        "1. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMQ9zGRd4ZbP"
      },
      "source": [
        "# Subtask 2.6\n",
        "## Save and restore model checkpoints\n",
        "\n",
        "Training that model for 100 epochs took quite a bit of time, right? Wouldn't it be a pity if it would get deleted out of memory, e.g. because your Colab session terminates (this can even happen automatically)? We would have to train it again to make predictions! To prevent this, we would like to save a check-point of the already optimized model's weights to disk. Then, we could just load our model weights at any time and use our model again without retraining. As you will see in a bit, this will be very handy for early stopping, too!\n",
        "\n",
        "#### TODO\n",
        "- Save a checkpoint of the `model` trained above (i.e. the model's parameters) to disk.\n",
        "- Initialize a new model, `model2` with the same architecture as used for the `model` you stored. Do *not* train `model2`.\n",
        "- Evaluate `model2` on the validation dataset.\n",
        "- Now, overwrite the initialized, untrained weights of `model2` with the weights you saved into the checkpoint of `model`.\n",
        "- Evaluate `model2`'s validation loss again. It should be of the exact same value as `model`'s validation loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsNvlq0H4ZbQ"
      },
      "source": [
        "*Hints:*\n",
        " - Read https://www.tensorflow.org/tutorials/keras/save_and_load\n",
        " - Use `model.save_weights`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z63x3LbM4ZbQ"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAR8MsuR4ZbQ"
      },
      "source": [
        "# Subtask 2.7\n",
        "## Early stopping\n",
        "So the model you ended up with after 100 epochs was not the best one. That has two implications for us: (1) We would not have had to train for that many epochs and could have saved some computing time. (2) We do not have the best model to apply our model to make actual predictions for unseen samples. If we would constantly assess our model's validation performance during training, we could stop optimization as soon as our model's performance does not increase anymore. This is called *early stopping*.\n",
        "\n",
        "### TODO\n",
        "- Implement Early Stopping using `tf.keras.callbacks.EarlyStopping`. Use patience = 5, which means that if the validation loss is not improved after 5 epochs, the training process should be stopped.\n",
        "- Save the model checkpoint after each epoch if the validation loss is improved. Use `tf.keras.callbacks.ModelCheckpoint`.\n",
        "- Train the model for 100 epochs with a learning rate of 0.01.\n",
        "- Load the model checkpoint.\n",
        "- Evaluate the model on the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba_Fm4024ZbQ"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_O4xy1x4ZbQ"
      },
      "outputs": [],
      "source": [
        "## TODO load the model checkpoint and evaluate on validation data\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPnjSQHt4ZbQ"
      },
      "source": [
        "#### TODO\n",
        "- Compare the training you just did with the one of the same model trained for 100 epochs. Did you reach best model performance? If so: why? If not: why not?\n",
        "- What is the purpose of `patience`, and why do we need that?\n",
        "- Do the same training as in the previous cell, starting training from scratch, but try different values for `patience` now. Did you end up with a model resulting in the best validation accuracy you have seen so far, but without training the full 100 epochs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ljlBrmS4ZbQ"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asEnQRrH4ZbQ"
      },
      "source": [
        "## Explore batch size *(optional)*\n",
        "\n",
        "*This task is optional, you do not need to solve it*\n",
        "\n",
        "Let us explore even more model and training parameters. In this section, we will see the impact of batch size on training. Let us use a learning rate of $10^{-3}$ from now on.\n",
        "\n",
        "\n",
        "#### TODO *(optional)*\n",
        "- Run training of the same model used above with\n",
        "    - batch size 1 for **one epoch**\n",
        "    - batch size 1024 for 100 epochs, using early stopping with patience 10\n",
        "- Compare your training results of all three batch sizes you have trained, i.e. batch size 1, 512 (from above) and 1024\n",
        "- Was it smart to set batch size to 1?\n",
        "- How long (in terms of computing time) do your models need to train for the different batch sizes? (You could even measure this with python, using the `time` package)\n",
        "- What is the impact on model performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su5Fc5Sf4ZbQ"
      },
      "outputs": [],
      "source": [
        "# Batch size 1\n",
        "\n",
        "###############################\n",
        "## YOUR CODE HERE - OPTIONAL ##\n",
        "###############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYik5ztl4ZbQ"
      },
      "outputs": [],
      "source": [
        "# Batch size 1024\n",
        "\n",
        "###############################\n",
        "## YOUR CODE HERE - OPTIONAL ##\n",
        "###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAy3p-aR4ZbQ"
      },
      "source": [
        "# Subtask 2.8\n",
        "## What about the architecture?\n",
        "\n",
        "How does architecture affect predictive performance?\n",
        "\n",
        "#### TODO:\n",
        "In the following, try to improve model performance by varying\n",
        "- number of hidden layers\n",
        "- number of neurons per each hidden layer\n",
        "- activation function\n",
        "\n",
        "These parameters are called hyper-parameters, since they are excluded from model optimization. Instead, we have to set them by hand and explore them to find a model with good predictive accuracy.\n",
        "\n",
        "Vary only one hyper-parameter at a time. If you would vary multiple parameters at the same time, it would be harder for you to see the impact that each parameter has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsuUCooz4ZbQ"
      },
      "outputs": [],
      "source": [
        "# number of hidden layers\n",
        "\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n",
        "\n",
        "# 4 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEI-nC6A4ZbQ"
      },
      "outputs": [],
      "source": [
        "# number of neurons per each hidden layer\n",
        "\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n",
        "\n",
        "# 100 neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tte0hcZ-4ZbQ"
      },
      "outputs": [],
      "source": [
        "# activation function\n",
        "\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n",
        "\n",
        "# Use tanh instead of ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HpGzX7D4ZbQ"
      },
      "source": [
        "**TODO Your answer here**\n",
        "\n",
        "1. How good do you get?\n",
        "2. Which hyper-parameter makes the largest difference?\n",
        "3. Does it always help to make your model bigger (i.e. wider / deeper)? Why not?\n",
        "\n",
        "**TODO Your answer here**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl19xAtu4ZbQ"
      },
      "source": [
        "**Your answers:**\n",
        "1. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFxpyd8O4ZbQ"
      },
      "source": [
        "# Subtask 2.9\n",
        "## Challenge\n",
        "#### TODO\n",
        "- If you choose your best values for number hidden units, number of layers and activation function that you determined by varying them independently above: Does performance improve? Why?\n",
        "- Vary all of the parameters at the same time to maximize the predictive performance of your model. How good do you get?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66GD625Q4ZbQ"
      },
      "outputs": [],
      "source": [
        "# Your best model:\n",
        "\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwsl3wNM4ZbQ"
      },
      "source": [
        "# Subtask 2.10\n",
        "## Evaluate your best model on test set, once!\n",
        "When doing a study, at the very end right before writing up your paper, you evaluate the best model you chose on the test set. This is the performance value you will report to the public.\n",
        "\n",
        "#### TODO\n",
        "- Evaluate the model on the testing dataset.\n",
        "- Plot the reference mass flow rate vs. the predicted values for the first 50 samples of the testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jHvz4jd4ZbR"
      },
      "outputs": [],
      "source": [
        "# TODO evaluate the model\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Fw4g3-K4ZbR"
      },
      "outputs": [],
      "source": [
        "# TODO plot results\n",
        "####################\n",
        "## YOUR CODE HERE ##\n",
        "####################\n",
        "\n",
        "y_pred = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiUlxYp34ZbR"
      },
      "source": [
        "**TODO Your answer here**\n",
        "\n",
        "1. Is the test loss of your model as good as the validation loss?\n",
        "2. If those values are different: How can you explain the difference?\n",
        "3. Why should you never use test set performance when trying out different hyper-parameters and architectures?\n",
        "\n",
        "**TODO Your answer here**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOzJH2Hg4ZbR"
      },
      "source": [
        "<span style='color:red'>**Your answer:**</span>\n",
        "\n",
        "1. ..."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "nteract": {
      "version": "0.15.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "062ad8061f6e7aca79bc9a27e986d47521568c0a72ae67729c623b905ab2cf31"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}